<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>A Matrix Factorization Approach to Linear Regression - The Pleasure of Finding Things Out</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">



<link rel="canonical" href="./a-matrix-factorization-approach-to-linear-regression.html">

        <meta name="author" content="James D. Triveri" />
        <meta name="keywords" content="Statistical Modeling,Python" />
        <meta name="description" content="A matrix factorization approach to linear regression" />

        <meta property="og:site_name" content="The Pleasure of Finding Things Out" />
        <meta property="og:type" content="article"/>
        <meta property="og:title" content="A Matrix Factorization Approach to Linear Regression"/>
        <meta property="og:url" content="./a-matrix-factorization-approach-to-linear-regression.html"/>
        <meta property="og:description" content="A matrix factorization approach to linear regression"/>
        <meta property="article:published_time" content="2023-04-23" />
            <meta property="article:section" content="Statistical Modeling" />
            <meta property="article:tag" content="Statistical Modeling" />
            <meta property="article:tag" content="Python" />
            <meta property="article:author" content="James D. Triveri" />



    <!-- Bootstrap -->
        <link rel="stylesheet" href="./theme/css/bootstrap.Yeti.min.css" type="text/css"/>
    <link href="./theme/css/font-awesome.min.css" rel="stylesheet">

    <link href="./theme/css/pygments/default.css" rel="stylesheet">
        <link href="./theme/css/typogrify.css" rel="stylesheet">
    <link rel="stylesheet" href="./theme/css/style.css" type="text/css"/>
        <link href="./static/css/custom.css" rel="stylesheet">




</head>
<body>

<div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
    <div class="container-fluid">
        <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-ex1-collapse">
                <span class="sr-only">Toggle navigation</span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
                <span class="icon-bar"></span>
            </button>
            <a href="./" class="navbar-brand">
The Pleasure of Finding Things Out            </a>
        </div>
        <div class="collapse navbar-collapse navbar-ex1-collapse">
            <ul class="nav navbar-nav">
                         <li><a href="./pages/about-me.html">
                             About&nbsp;Me
                          </a></li>
                        <li >
                            <a href="./category/machine-learning.html">Machine learning</a>
                        </li>
                        <li >
                            <a href="./category/python.html">Python</a>
                        </li>
                        <li >
                            <a href="./category/r.html">R</a>
                        </li>
                        <li class="active">
                            <a href="./category/statistical-modeling.html">Statistical modeling</a>
                        </li>
            </ul>
            <ul class="nav navbar-nav navbar-right">
            </ul>
        </div>
        <!-- /.navbar-collapse -->
    </div>
</div> <!-- /.navbar -->

<!-- Banner -->
<!-- End Banner -->

<!-- Content Container -->
<div class="container-fluid">
    <div class="row">
        <div class="col-sm-9">
    <section id="content">
        <article>
            <header class="page-header">
                <h1>
                    <a href="./a-matrix-factorization-approach-to-linear-regression.html"
                       rel="bookmark"
                       title="Permalink to A Matrix Factorization Approach to Linear Regression">
                        A Matrix Factorization Approach to Linear&nbsp;Regression
                    </a>
                </h1>
            </header>
            <div class="entry-content">
                <div class="panel">
                    <div class="panel-body">
<footer class="post-info">
    <span class="label label-default">Date</span>
    <span class="published">
        <i class="fa fa-calendar"></i><time datetime="2023-04-23T00:00:00-05:00"> 2023-04-23</time>
    </span>





<span class="label label-default">Tags</span>
	<a href="./tag/statistical-modeling.html">Statistical Modeling</a>
        /
	<a href="./tag/python.html">Python</a>
    
</footer><!-- /.post-info -->                    </div>
                </div>
                <p>This post is primarily intended to shed light on why the closed form solution to linear regression estimates is avoided in 
statistical software packages. But we start by first by deriving the solution to the normal equations within the standard 
multivariate regression&nbsp;setting.</p>
<p>The normal linear regression model specifies that the sampling variability around the mean is i.i.d. from a normal&nbsp;distribution:</p>
<p>$$
\epsilon_{1}, \cdots, \epsilon_{n} \sim \mathrm{i.i.d.}\hspace{.25em}\mathrm{normal}(0, \sigma^{2})\
y_{i} = \beta^{T}x_{i} + \epsilon_{i}.&nbsp;$$</p>
<p>Therefore the specification for the joint <span class="caps">PDF</span> of observed data conditional upon and values   and is given&nbsp;by:</p>
<p>$$
\begin{align<em>} 
f(y_{1}, \cdots, y_{n}|x_{1}, \cdots, x_{n}, \beta, \sigma^{2}) &amp;= \prod_{i=1}^{n} f(y_{i}|x_{i}, \beta, \sigma^{2})\
&amp;= (2\pi \sigma^{2})^{-n/2} \mathrm{exp}\big(-\frac{1}{2\sigma^{2}} \sum_{i=1}^{n}(y_{i} - \beta^{T}x_{i})^{2}\big).
\end{align</em>}&nbsp;$$</p>
<p>Alternatively, the joint <span class="caps">PDF</span> can be represented in terms of the multivariate normal distribution. Let $y$ be n-dimensional
response vector, and $X$ the $n \times p$ design matrix whose $i^{th}$ row is $x_{i}$. We&nbsp;have:</p>
<p>$$
y|X,\beta, \sigma^{2} \sim \mathrm{<span class="caps">MVN</span>}(X\beta, \sigma^{2} \mathrm{I})&nbsp;$$</p>
<p>Where $I$ represents the $p \times p$ identity&nbsp;matrix.</p>
<p>The density depends on $\beta$ through the residuals. Given the observed data, the term in the exponent
$(y_{i} - \beta^{T}x_{i})$ is maximized when the sum of squared residuals, 
$\mathrm{<span class="caps">SSR</span>} = \sum_{i=1}^{n} (y_{i} - \beta^{T}x_{i})$ is minimized. To find the 
optimal $\beta$, we expand the expression for the residual sum of&nbsp;squares:</p>
<p>$$
\begin{align<em>} 
\mathrm{<span class="caps">SSR</span>} &amp;= \sum_{i=1}^{n} (y_{i} - \beta^{T}x_{i})\
&amp;=(y - X\beta)^{T}(y - X\beta)\
&amp;=y^{T}y - 2\beta^{T}X^{T}y + \beta^{T}X^{T}X\beta.
\end{align</em>}&nbsp;$$</p>
<p>Computing the first derivative of the last expression above w.r.t.  $\beta$ and setting equal to 0 yields
$-2X^{T}y + 2X{T}X\beta = 0$, which can be rearranged to obtain the familiar expression for the ordinary least 
squares&nbsp;solution:</p>
<p>$$
\beta = (X^{T}X)^{-1}X^{T}y.&nbsp;$$</p>
<p>Why isn&#8217;t this expression implemented in linear model solvers&nbsp;directly?   </p>
<p>The condition number of a matrix is the ratio of maximum-to-minimum singular values (which, for a normal matrix, is 
the ratio of the maximum-to-minimum absolute value of eigenvalues). Essentially, the condition number tells you how 
much solving a linear system will magnify any noise in your data. It can be thought of as a measure of amplification. 
The smaller the condition number, the better (the best value being&nbsp;1).</p>
<div class="highlight"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Demonstrating the equivalence of computing the condition</span>
<span class="sd">number as the ratio of maximum-to-minimum singular values</span>
<span class="sd">and using np.linalg.cond, as well as a comparison of the </span>
<span class="sd">condition numbers for X vs. X^T*X. </span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">516</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

<span class="c1"># SVD for X. </span>
<span class="n">U0</span><span class="p">,</span> <span class="n">S0</span><span class="p">,</span> <span class="n">Vt0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">c0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># SVD for X^T*X. </span>
<span class="n">U1</span><span class="p">,</span> <span class="n">S1</span><span class="p">,</span> <span class="n">Vt1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">c1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cond</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>

<span class="c1"># S0 and S1 represent the singular values of X and X^T*X.</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;S0.max() / S0.min()       : </span><span class="si">{</span><span class="n">S0</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">S0</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">:</span><span class="s2">.8f</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Condition number of X     : </span><span class="si">{</span><span class="n">c0</span><span class="si">:</span><span class="s2">.8f</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;S1.max() / S1.min()       : </span><span class="si">{</span><span class="n">S1</span><span class="o">.</span><span class="n">max</span><span class="p">()</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">S1</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">:</span><span class="s2">.8f</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Condition number of X^T*X : </span><span class="si">{</span><span class="n">c1</span><span class="si">:</span><span class="s2">.8f</span><span class="si">}</span><span class="s2">.&quot;</span><span class="p">)</span>

<span class="c1"># S0.max() / S0.min()       : 2.44498390.</span>
<span class="c1"># Condition number of X     : 2.44498390.</span>
<span class="c1"># S1.max() / S1.min()       : 5.97794628.</span>
<span class="c1"># Condition number of X^T*X : 5.97794628.</span>
</code></pre></div>

<p>In terms of numerical precision, computing $X^{T}X$ roughly squares the condition number. As an approximation, 
$\mathrm{log}_{10}(\mathrm{condition})$ represents the number of digits lost in a given matrix computation. So by merely 
forming the Gram matrix, we&#8217;ve doubled the loss of precision in our final result,&nbsp;since </p>
<p>$$
\mathrm{log}<em 10>{10}(\mathrm{condition}(X^{T}X)) \approx 2 \times \mathrm{log}</em>(\mathrm{condition}(X)).&nbsp;$$</p>
<p>If the condition number of $X$ is small, forming the Gram matrix and solving the 
system via $\beta = (X^{T}X)^{-1}X^{T}y$ should be fine. But as the condition 
number grows, solving the normal equations becomes increasingly unstable, ultimately 
resulting in a solution devoid of accuracy. Since statistical software packages 
need to handle an incredible variety of potential design matrices with a wide range 
of condition numbers, the normal equations approach cannot be relied&nbsp;upon.</p>
<h2>The <span class="caps">QR</span>&nbsp;Decomposition</h2>
<p>The <span class="caps">QR</span> Decomposition factors a matrix $X$ into the product of an orthonormal matrix $Q$ 
and an upper triangular matrix $R$, $X = <span class="caps">QR</span>$. Because $Q$ is orthonormal, $Q^{T} = Q^{-1}$. 
Beginning with a version of the normal equations solution, substitute $<span class="caps">QR</span>$ for $X$, 
rearrange and arrive&nbsp;at:</p>
<p>$$
\begin{align<em>} 
X^{T}X \beta &amp;= X^{T}y\
(<span class="caps">QR</span>)^{T}(<span class="caps">QR</span>) \beta &amp;= (<span class="caps">QR</span>)^{T}y\
R^{T}(Q^{T}Q)R \beta &amp;= R^{T} Q^{T} y\
R^{T}R \beta &amp;= R^{T} Q^{T} y\
(R^{T})^{-1} R^{T} R \beta &amp;= (R^{T})^{-1} R^{T} Q^{T} y\
R \beta &amp;= Q^{T} y,
\end{align</em>}&nbsp;$$</p>
<p>where we&#8217;ve taken advantage of how transpose distributes over matrix products (i.e. $(<span class="caps">AB</span>)^{T} = B^{T}A^{T}$),
and the fact that since is orthonormal, $Q^{T}Q =&nbsp;I$.</p>
<p>Because $R$ is upper triangular, $\beta$ can be solved for using back substitution. A quick demonstration of 
how this can be accomplished in&nbsp;Python:</p>
<div class="highlight"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Solving for regression coefficents using normal equations </span>
<span class="sd">and QR decomposition. </span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy.linalg</span> <span class="kn">import</span> <span class="n">solve_triangular</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">516</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="c1"># Normal equations solution.</span>
<span class="n">B0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>

<span class="c1"># QR Decomposition solution.</span>
<span class="n">Q</span><span class="p">,</span> <span class="n">R</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">qr</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;reduced&quot;</span><span class="p">)</span>
<span class="n">B1</span> <span class="o">=</span> <span class="n">solve_triangular</span><span class="p">(</span><span class="n">R</span><span class="p">,</span> <span class="n">Q</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;B0: </span><span class="si">{</span><span class="n">B0</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;B1: </span><span class="si">{</span><span class="n">B1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;np.allclose(B0, B1): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">B0</span><span class="p">,</span><span class="w"> </span><span class="n">B1</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># B0: [ 0.42402765 -1.21951527  0.22396056  0.26773935 -0.72067314]</span>
<span class="c1"># B1: [ 0.42402765 -1.21951527  0.22396056  0.26773935 -0.72067314]</span>
<span class="c1"># np.allclose(B0, B1): True</span>
</code></pre></div>

<p>Using the <span class="caps">QR</span> Decomposition, we&#8217;ve eliminated the need to explicitly create the 
Gram matrix, and no longer need to invert matrices, which is computationally expensive 
and has its own set of precision degradation&nbsp;issues.</p>
<h2>The Singular Value&nbsp;Decomposition</h2>
<p>The Singular Value Decomposition is a generalization of the Eigendecomposition 
to any $nxp$ matrix. The <span class="caps">SVD</span> decomposes a matrix $A$ into 3 matrices ($A = U \Sigma&nbsp;V^{T}$):</p>
<ul>
<li>$U$ is a $n \times p$ orthogonal matrix (assuming $A$ is real); columns represent <em>left singular vectors</em>.</li>
<li>$\Sigma$ is a $p \times p$ diagonal matrix with diagonal entries representing the <em>singular values</em> of&nbsp;$A$.</li>
<li>$V^{T}$ is a $p \times p$ orthogonal matrix (assuming $A$ is real); rows represent <em>right singular vectors</em>.</li>
</ul>
<p>Beginning with the normal equations solution, replace $X$ with $U \Sigma V^{T}$ and solve for&nbsp;$\beta$:</p>
<p>$$
\begin{align<em>} 
X^{T}X \beta &amp;= X^{T}y\
(U \Sigma V^{T})^{T}U \Sigma V^{T}\beta &amp;= (U \Sigma V^{T})^{T}y\
V \Sigma^{T} U^{T} U \Sigma V^{T} \beta &amp;= V \Sigma U^{T} y\
V \Sigma^{T} \Sigma V^{T} \beta &amp;=V \Sigma U^{T} y\
V V^{T} \beta &amp;= V \Sigma^{-1} U^{T} y\
\beta &amp;= V \Sigma^{-1} U^{T} y
\end{align</em>}&nbsp;$$</p>
<p>Since $\Sigma$ is a diagonal matrix, the inverse is simply the reciprocal of the diagonal elements, which doesn&#8217;t incur 
the runtime associated with a conventional matrix inversion. In addition, $<span class="caps">VV</span>^{T} = I$. Note that we assume the singular
values are strictly greater than 0. If this is not the case, the condition number would be infinite, and a well-defined 
solution wouldn&#8217;t&nbsp;exist.</p>
<p>Determining the estimated coefficients using <span class="caps">SVD</span> in Python can be accomplished as&nbsp;follows:</p>
<div class="highlight"><pre><span></span><code><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Solving for regression coefficents using normal equations </span>
<span class="sd">and SVD. </span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="mi">516</span><span class="p">)</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">scale</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>

<span class="c1"># Normal equations solution.</span>
<span class="n">B0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">inv</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">X</span><span class="p">)</span> <span class="o">@</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>

<span class="c1"># SVD solution.</span>
<span class="n">U</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">Vt</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">svd</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">full_matrices</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="n">B1</span> <span class="o">=</span> <span class="n">Vt</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">np</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">S</span><span class="p">)</span> <span class="o">@</span> <span class="n">U</span><span class="o">.</span><span class="n">T</span> <span class="o">@</span> <span class="n">y</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;B0: </span><span class="si">{</span><span class="n">B0</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;B1: </span><span class="si">{</span><span class="n">B1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;np.allclose(B0, B1): </span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">B0</span><span class="p">,</span><span class="w"> </span><span class="n">B1</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="n">B0</span><span class="p">:</span> <span class="p">[</span> <span class="mf">0.42402765</span> <span class="o">-</span><span class="mf">1.21951527</span>  <span class="mf">0.22396056</span>  <span class="mf">0.26773935</span> <span class="o">-</span><span class="mf">0.72067314</span><span class="p">]</span>
<span class="n">B1</span><span class="p">:</span> <span class="p">[</span> <span class="mf">0.42402765</span> <span class="o">-</span><span class="mf">1.21951527</span>  <span class="mf">0.22396056</span>  <span class="mf">0.26773935</span> <span class="o">-</span><span class="mf">0.72067314</span><span class="p">]</span>
<span class="n">np</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">B0</span><span class="p">,</span> <span class="n">B1</span><span class="p">):</span> <span class="kc">True</span>
</code></pre></div>

<p>Each of these methods (normal equations, <span class="caps">QR</span> Decomposition, <span class="caps">SVD</span>) incur different computational cost. 
From Golub <span class="amp">&amp;</span> Van Loan, the flop count associated with each algorithm is presented&nbsp;below:</p>
<div class="highlight"><pre><span></span><code><span class="nb">-------------------------------------------</span>
<span class="c">Normal Equations       |  mn^2 </span><span class="nb">+</span><span class="c"> n^3 / 3  |</span>
<span class="nb">-------------------------------------------</span>
<span class="c">Householder QR         |  n^3 / 3         |</span>
<span class="nb">-------------------------------------------</span>
<span class="c">Modified Gram</span><span class="nb">-</span><span class="c">Schmidt  |  2mn^2           |</span>
<span class="nb">-------------------------------------------</span>
<span class="c">SVD                    |  2mn^2 </span><span class="nb">+</span><span class="c"> 11n^3   |</span>
<span class="nb">-------------------------------------------</span>
</code></pre></div>

<p>Householder and Modified Gram-Schmidt are two approaches used in the first step of the <span class="caps">QR</span> Decomposition. <span class="caps">SVD</span> offers far 
more stability, but comes with added runtime complexity. Other matrix decomposition methods such as <span class="caps">LU</span> and Cholesky can 
be leveraged, but the <span class="caps">QR</span> Decomposition represents a good trade-off between runtime and numerical stability. This 
explains its wide use in statistical software packages. Check out the excellent <a href="http://madrury.github.io/jekyll/update/statistics/2016/07/20/lm-in-R.html">A Deep Dive into how R Fits a Linear Model</a> 
for an in-depth explanation of how the <span class="caps">QR</span> Decomposition is used to fit linear models in&nbsp;R.</p>
            </div>
            <!-- /.entry-content -->
        </article>
    </section>

        </div>
        <div class="col-sm-3" id="sidebar">
            <aside>
<div id="aboutme">
        <p>
            <img width="100%" class="img-thumbnail" src="./images/JDTGOOG.JPG"/>
        </p>
    <p>
      <strong>About James D. Triveri</strong><br/>
        Data Scientist interested in ML/DL, automation and scientific computing.
    </p>
</div><!-- Sidebar -->
<section class="well well-sm">
  <ul class="list-group list-group-flush">

<!-- Sidebar/Links -->
<li class="list-group-item">
  <h4><i class="fa fa-external-link-square fa-lg"></i><span class="icon-label">Links</span></h4>
  <ul class="list-group" id="links">
    <li class="list-group-item">
      <a href="http://python.org/" target="_blank">Python.org</a>
    </li>
    <li class="list-group-item">
      <a href="https://docs.python.org/3/py-modindex.html" target="_blank">The Python Module Index</a>
    </li>
    <li class="list-group-item">
      <a href="https://scikit-learn.org/stable/documentation.html" target="_blank">Scikit-Learn</a>
    </li>
    <li class="list-group-item">
      <a href="https://www.scipy.org/docs.html" target="_blank">Scipy Docs</a>
    </li>
    <li class="list-group-item">
      <a href="https://openai.com/" target="_blank">OpenAI</a>
    </li>
  </ul>
</li>
<!-- End Sidebar/Links -->

<!-- Sidebar/Images -->
<li class="list-group-item">
  <ul class="list-group" id="links">
    <img width="100%" class="img-thumbnail" src="images/Sidebar/sidebarA.jpg"/>
    <img width="100%" class="img-thumbnail" src="images/Sidebar/sidebarB.png"/>
    <img width="100%" class="img-thumbnail" src="images/Sidebar/sidebarC.jpg"/>
    <img width="100%" class="img-thumbnail" src="images/Sidebar/sidebarD.jpg"/>
  </ul>
</li>
<!-- End Sidebar/Images -->
  </ul>
</section>
<!-- End Sidebar -->            </aside>
        </div>
    </div>
</div>
<!-- End Content Container -->

<footer>
   <div class="container-fluid">
      <hr>
      <div class="row">
         <div class="col-xs-10">&copy; 2023 James D. Triveri
            &middot; Powered by <a href="https://github.com/getpelican/pelican-themes/tree/master/pelican-bootstrap3" target="_blank">pelican-bootstrap3</a>,
            <a href="http://docs.getpelican.com/" target="_blank">Pelican</a>,
            <a href="http://getbootstrap.com" target="_blank">Bootstrap</a>         </div>
         <div class="col-xs-2"><p class="pull-right"><i class="fa fa-arrow-up"></i> <a href="#">Back to top</a></p></div>
      </div>
   </div>
</footer>
<script src="./theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="./theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="./theme/js/respond.min.js"></script>


    <script src="./theme/js/bodypadding.js"></script>


</body>
</html>